{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNrBe41Hi+l8DKCxWZpEhku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/masoudcharkhabi/ML-from-Expert-Preferences/blob/colab/baseline/experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "zuDTEufZrEYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make sure you have a GPU and High memory"
      ],
      "metadata": {
        "id": "FBnUlJkFDZc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v10tOBD5rFcA",
        "outputId": "3a2b3ec9-d3aa-4ecc-dcc7-b99a884dcb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Dec  3 04:00:29 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rnSFGIj5APcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drive mount and SSH keys (id_colab) for Github"
      ],
      "metadata": {
        "id": "PHMCnbkcCdFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create symbolic links to the SSH keys in Drive\n",
        "!ln -s /content/drive/MyDrive/ssh_keys/id_colab ~/.ssh/id_colab\n",
        "!ln -s /content/drive/MyDrive/ssh_keys/id_colab.pub ~/.ssh/id_colab.pub\n",
        "\n",
        "# Set the correct permissions for SSH keys\n",
        "!chmod 600 ~/.ssh/id_colab\n",
        "!chmod 644 ~/.ssh/id_colab.pub\n",
        "\n",
        "# Start SSH agent and add key\n",
        "!eval \"$(ssh-agent -s)\"\n",
        "!ssh-add ~/.ssh/id_colab\n",
        "\n",
        "# Create SSH config\n",
        "ssh_config = \"\"\"\n",
        "Host github.com\n",
        "  HostName github.com\n",
        "  User git\n",
        "  IdentityFile ~/.ssh/id_colab\n",
        "  StrictHostKeyChecking no\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.expanduser(\"~/.ssh/config\"), \"w\") as f:\n",
        "    f.write(ssh_config)\n",
        "\n",
        "# Test SSH connection\n",
        "!ssh -T git@github.com"
      ],
      "metadata": {
        "id": "1egWyduzASgZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVcW1-YK-B-3",
        "outputId": "c54be1c6-8688-4c8b-a67d-353278715494"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone Github repo"
      ],
      "metadata": {
        "id": "401jtOTODD7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_ssh_url = \"git@github.com:masoudcharkhabi/ML-from-Expert-Preferences.git\"\n",
        "branch_name = \"colab\"\n",
        "!git clone -b {branch_name} --single-branch {repo_ssh_url}"
      ],
      "metadata": {
        "id": "dMUqzX1JrILL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "585466eb-7314-447a-e9f8-f50e26b08785"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML-from-Expert-Preferences'...\n",
            "remote: Enumerating objects: 175, done.\u001b[K\n",
            "remote: Counting objects: 100% (175/175), done.\u001b[K\n",
            "remote: Compressing objects: 100% (127/127), done.\u001b[K\n",
            "remote: Total 175 (delta 85), reused 110 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (175/175), 314.28 KiB | 467.00 KiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"ML-from-Expert-Preferences\"\n",
        "os.chdir(repo_name)"
      ],
      "metadata": {
        "id": "gF1ieVq3BJj1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git branch\n",
        "!ls -ltr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afxaCnjJBihb",
        "outputId": "8a06337d-9858-4bd3-a508-8aa7aeee015b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* \u001b[32mcolab\u001b[m\n",
            "total 20\n",
            "-rw-r--r-- 1 root root 2680 Dec  3 05:35 README.md\n",
            "drwxr-xr-x 4 root root 4096 Dec  3 05:35 cs329h-project\n",
            "drwxr-xr-x 2 root root 4096 Dec  3 05:35 baseline\n",
            "drwxr-xr-x 5 root root 4096 Dec  3 05:35 data\n",
            "-rw-r--r-- 1 root root  155 Dec  3 05:35 requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda create --name cs329h-project --file requirements.txt python=3.9 -c conda-forge"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYNMV0vHFyG7",
        "outputId": "6758e71f-2044-4d5e-e251-871ff3c3c55b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine-tune the models"
      ],
      "metadata": {
        "id": "GLmHG-UzFvYr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run fine_tune.py"
      ],
      "metadata": {
        "id": "usCJ8oyuFPDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python baseline/fine_tune.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGiGYDMDFVbB",
        "outputId": "d6148368-1169-474b-d49e-bcdb20615991"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-03 05:55:06.358809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 05:55:06.379736: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 05:55:06.386290: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 05:55:06.402315: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 05:55:07.571157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ML-from-Expert-Preferences/baseline/fine_tune.py\", line 6, in <module>\n",
            "    from datasets import load_dataset\n",
            "ModuleNotFoundError: No module named 'datasets'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yMtc3P3BFBas"
      }
    }
  ]
}